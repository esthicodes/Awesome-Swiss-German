{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Document-Distance\" data-toc-modified-id=\"Document-Distance-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Document Distance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Text-Re-Use\" data-toc-modified-id=\"Text-Re-Use-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Text Re-Use</a></span></li><li><span><a href=\"#Cosine-Similarity\" data-toc-modified-id=\"Cosine-Similarity-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Cosine Similarity</a></span></li><li><span><a href=\"#Jensen-Shannon-Divergence\" data-toc-modified-id=\"Jensen-Shannon-Divergence-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Jensen-Shannon Divergence</a></span></li></ul></li><li><span><a href=\"#Clustering\" data-toc-modified-id=\"Clustering-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#K-means-clustering\" data-toc-modified-id=\"K-means-clustering-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>K-means clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Silhouette-Score\" data-toc-modified-id=\"Silhouette-Score-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Silhouette Score</a></span></li></ul></li><li><span><a href=\"#K-Medoids\" data-toc-modified-id=\"K-Medoids-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>K-Medoids</a></span></li><li><span><a href=\"#DBSCAN\" data-toc-modified-id=\"DBSCAN-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>DBSCAN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hierarchical-DBSCAN\" data-toc-modified-id=\"Hierarchical-DBSCAN-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Hierarchical DBSCAN</a></span></li></ul></li><li><span><a href=\"#Hierarchical-Clustering\" data-toc-modified-id=\"Hierarchical-Clustering-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Hierarchical Clustering</a></span></li><li><span><a href=\"#Principal-Component-Analysis\" data-toc-modified-id=\"Principal-Component-Analysis-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Principal Component Analysis</a></span></li></ul></li><li><span><a href=\"#Latent-Dirichlet-Allocation\" data-toc-modified-id=\"Latent-Dirichlet-Allocation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Latent Dirichlet Allocation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Singular-Value-Decomposition-(SVD)\" data-toc-modified-id=\"Singular-Value-Decomposition-(SVD)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Singular Value Decomposition (SVD)</a></span></li><li><span><a href=\"#Non-negative-Matrix-Factorization-(NMF)\" data-toc-modified-id=\"Non-negative-Matrix-Factorization-(NMF)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Non-negative Matrix Factorization (NMF)</a></span></li><li><span><a href=\"#Author-Topic-Model\" data-toc-modified-id=\"Author-Topic-Model-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Author Topic Model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing for Law and Social Science<br>\n",
    "Directed by Elliott Ash, ETH Zurich, edited by Estheryu991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T09:40:51.192954Z",
     "start_time": "2022-03-11T09:40:51.186493Z"
    }
   },
   "outputs": [],
   "source": [
    "# set random seed\n",
    "import numpy as np\n",
    "np.random.seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T09:40:51.740668Z",
     "start_time": "2022-03-11T09:40:51.614198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "df = pd.read_pickle('sc_cases_cleaned.pkl',compression='gzip')\n",
    "X = pd.read_pickle('X.pkl').toarray()\n",
    "X_tfidf = pd.read_pickle('X_tfidf.pkl').toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T09:40:52.236650Z",
     "start_time": "2022-03-11T09:40:52.209792Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "text0 = ' '.join(simple_preprocess(df['opinion_text'][0]))\n",
    "text1 = ' '.join(simple_preprocess(df['opinion_text'][1]))\n",
    "\n",
    "text1[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Re-Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on this implementation of the Smith-Waterman algorithm can be found [here](https://tiefenauer.github.io/blog/smith-waterman/#step-1-scoring-matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T09:41:24.617382Z",
     "start_time": "2022-03-11T09:41:23.676571Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def matrix(a, b, match_score=3, gap_cost=2):\n",
    "    H = np.zeros((len(a) + 1, len(b) + 1), np.int)\n",
    "\n",
    "    for i, j in itertools.product(range(1, H.shape[0]), range(1, H.shape[1])):\n",
    "        match = H[i - 1, j - 1] + (match_score if a[i - 1] == b[j - 1] else - match_score)\n",
    "        delete = H[i - 1, j] - gap_cost\n",
    "        insert = H[i, j - 1] - gap_cost\n",
    "        H[i, j] = max(match, delete, insert, 0)\n",
    "    return H\n",
    "def traceback(H, b, b_='', old_i=0):\n",
    "    # flip H to get index of **last** occurrence of H.max() with np.argmax()\n",
    "    H_flip = np.flip(np.flip(H, 0), 1)\n",
    "    i_, j_ = np.unravel_index(H_flip.argmax(), H_flip.shape)\n",
    "    i, j = np.subtract(H.shape, (i_ + 1, j_ + 1))  # (i, j) are **last** indexes of H.max()\n",
    "    if H[i, j] == 0:\n",
    "        return b_, j\n",
    "    b_ = b[j - 1] + '-' + b_ if old_i - i > 1 else b[j - 1] + b_\n",
    "    return traceback(H[0:i, 0:j], b, b_, i)\n",
    "def smith_waterman(a, b, match_score=3, gap_cost=2):\n",
    "    a, b = a.upper(), b.upper()\n",
    "    H = matrix(a, b, match_score, gap_cost)\n",
    "    b_, pos = traceback(H, b)\n",
    "    return pos, pos + len(b_)\n",
    "\n",
    "start, end = smith_waterman(text0[:1000], text1[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T09:41:24.865349Z",
     "start_time": "2022-03-11T09:41:24.852460Z"
    }
   },
   "outputs": [],
   "source": [
    "text0[start: end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T09:41:43.952894Z",
     "start_time": "2022-03-11T09:41:43.919280Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute pair-wise similarities between all documents in corpus\"\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim = cosine_similarity(X[:100])\n",
    "sim.shape\n",
    "\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T09:41:49.366523Z",
     "start_time": "2022-03-11T09:41:49.344155Z"
    }
   },
   "outputs": [],
   "source": [
    "# TF-IDF Similarity\n",
    "tsim = cosine_similarity(X[:100])\n",
    "tsim[:3,:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jensen-Shannon Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T08:38:11.995406Z",
     "start_time": "2022-03-11T08:38:11.985278Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "def js(p, q):\n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "    m = (p + q) / 2\n",
    "    return (entropy(p, m) + entropy(q, m)) / 2\n",
    "js(tsim[0],tsim[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T09:43:29.382083Z",
     "start_time": "2022-03-11T09:43:25.856420Z"
    }
   },
   "outputs": [],
   "source": [
    "# create 100 clusters of similar documents\n",
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 40\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(X)\n",
    "doc_clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T09:43:55.174072Z",
     "start_time": "2022-03-11T09:43:55.149135Z"
    }
   },
   "outputs": [],
   "source": [
    "df['cluster'] = doc_clusters\n",
    "df[df['cluster']==3]['opinion_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score\n",
    "\n",
    "Choose the optimal number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T09:44:52.277204Z",
     "start_time": "2022-03-11T09:44:52.141635Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(X, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T09:46:45.296285Z",
     "start_time": "2022-03-11T09:45:12.878930Z"
    }
   },
   "outputs": [],
   "source": [
    "sil_scores = []\n",
    "for n in range(2, num_clusters):\n",
    "    km = KMeans(n_clusters=n)\n",
    "    km.fit(X)\n",
    "    sil_scores.append(silhouette_score(X, km.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T09:46:45.467824Z",
     "start_time": "2022-03-11T09:46:45.343903Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(range(2, num_clusters), sil_scores)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:17.926048Z",
     "start_time": "2022-03-11T07:07:17.917927Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_sil_score = max(sil_scores[5:20])\n",
    "sil_scores.index(opt_sil_score)\n",
    "opt_num_cluster = range(2, num_clusters)[sil_scores.index(opt_sil_score)]\n",
    "print('The optimal number of clusters is %s' %opt_num_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:19.322909Z",
     "start_time": "2022-03-11T07:07:17.934484Z"
    }
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=opt_num_cluster)\n",
    "km.fit(X)\n",
    "doc_clusters = km.labels_.tolist()\n",
    "\n",
    "df['cluster_mean'] = doc_clusters\n",
    "df[df['cluster_mean']==1]['opinion_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:19.388613Z",
     "start_time": "2022-03-11T07:07:19.340115Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install sklearn_extra\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "kmed = KMedoids(n_clusters=opt_num_cluster)\n",
    "kmed.fit(X)\n",
    "doc_clusters = kmed.labels_.tolist()\n",
    "\n",
    "df['cluster_med'] = doc_clusters\n",
    "df[df['cluster_med']==1]['opinion_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:19.439357Z",
     "start_time": "2022-03-11T07:07:19.391327Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.95, min_samples=5)\n",
    "dbscan.fit(X_tfidf)\n",
    "db_clusters = dbscan.labels_\n",
    "\n",
    "df['cluster_db'] = db_clusters\n",
    "df[df['cluster_db']==1]['opinion_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical DBSCAN\n",
    "\n",
    "Automatically chooses epsilon, performing DBSCAN over various epsilon values e returns the result that gives the best stability over epsilon. For reference see [here](https://github.com/scikit-learn-contrib/hdbscan/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:20.304382Z",
     "start_time": "2022-03-11T07:07:19.445785Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install hdbscan\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan = HDBSCAN(min_cluster_size=5)\n",
    "hdbscan.fit(X_tfidf)\n",
    "hdb_clusters = hdbscan.labels_\n",
    "\n",
    "df['cluster_hdb'] = hdb_clusters\n",
    "df[df['cluster_hdb']==1]['opinion_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:20.394867Z",
     "start_time": "2022-03-11T07:07:20.305217Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=opt_num_cluster, affinity='euclidean', linkage='ward')\n",
    "cluster.fit_predict(X)\n",
    "\n",
    "clusters = dbscan.labels_\n",
    "\n",
    "df['cluster_hie'] = clusters\n",
    "df[df['cluster_hie']==1]['opinion_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:20.548510Z",
     "start_time": "2022-03-11T07:07:20.395612Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% Principal Components\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3,svd_solver='randomized')\n",
    "Xpca = pca.fit_transform(X)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:20.624942Z",
     "start_time": "2022-03-11T07:07:20.563819Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% PCA Viz\n",
    "plt.scatter(Xpca[:,0],Xpca[:,1], alpha=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:20.771563Z",
     "start_time": "2022-03-11T07:07:20.636987Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% PCA 3D Viz\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "Axes3D(plt.figure()).scatter(Xpca[:,0],Xpca[:,1], Xpca[:,2], alpha=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:21.644782Z",
     "start_time": "2022-03-11T07:07:20.780568Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% make components to explain 95% of variance\n",
    "pca = PCA(n_components=.95)\n",
    "X95 = pca.fit_transform(X)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:21.792302Z",
     "start_time": "2022-03-11T07:07:21.645487Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% PCA Inverse Transform\n",
    "Xrestore = pca.inverse_transform(X95)\n",
    "plt.plot(Xrestore[0],X[0],'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:32.071529Z",
     "start_time": "2022-03-11T07:07:21.816551Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% Incremental PCA\n",
    "X_mm = np.memmap('X.pkl',shape=(32567, 525))\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "inc_pca = IncrementalPCA(n_components=100, batch_size=1000)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:32.079855Z",
     "start_time": "2022-03-11T07:07:32.072452Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% PC Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "Y = df['log_cite_count']\n",
    "lin_reg = LinearRegression()\n",
    "scores = cross_val_score(lin_reg,\n",
    "                         X95[:,:10],\n",
    "                         Y) \n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:36.448807Z",
     "start_time": "2022-03-11T07:07:32.082221Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% MDS, Isomap, and T-SNE\n",
    "from sklearn.manifold import MDS, Isomap, TSNE\n",
    "mds = MDS(n_components=2)\n",
    "Xmds = mds.fit_transform(X[:500,:200])\n",
    "Axes3D(plt.figure()).scatter(Xmds[:,0],Xmds[:,1], alpha=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:36.765819Z",
     "start_time": "2022-03-11T07:07:36.472825Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% Isomap\n",
    "iso = Isomap(n_components=2)\n",
    "Xiso = iso.fit_transform(X[:500,:200])\n",
    "Axes3D(plt.figure()).scatter(Xiso[:,0],Xiso[:,1], alpha=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:38.158032Z",
     "start_time": "2022-03-11T07:07:36.781207Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% t-SNE\n",
    "tsne = TSNE(n_components=2, n_iter=250)\n",
    "Xtsne = tsne.fit_transform(X[:500,:200])\n",
    "Axes3D(plt.figure()).scatter(Xtsne[:,0],Xtsne[:,1], alpha=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "For further reference see the material from topic [modeling with gensim](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T08:04:52.181598Z",
     "start_time": "2022-03-11T08:02:37.752474Z"
    }
   },
   "outputs": [],
   "source": [
    "# clean document\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from tqdm import tqdm as tq\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# this is faster and we don't need the whole grammatical parse analysis\n",
    "\n",
    "def tokenize(x, nlp):\n",
    "    # lemmatize and lowercase without stopwords, punctuation and numbers\n",
    "    return [w.lemma_.lower() for w in nlp(x) if not w.is_stop and not w.is_punct and not w.is_digit and len(w) > 2]\n",
    "\n",
    "# split into paragraphs\n",
    "doc_clean = []\n",
    "for doc in tq(df['opinion_text'][:100]):\n",
    "    # split by paragraph\n",
    "    for paragraph in doc.split(\"\\n\\n\"):\n",
    "        doc_clean.append(tokenize(paragraph, nlp))\n",
    "print (doc_clean[:10])\n",
    "\n",
    "\n",
    "# randomize document order\n",
    "from random import shuffle\n",
    "shuffle(doc_clean)\n",
    "\n",
    "# creating the term dictionary\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "# filter extremes, drop all words appearing in less than 10 paragraphs and all words appearing in at least every third paragraph\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.33, keep_n=1000)\n",
    "print (len(dictionary))\n",
    "\n",
    "\n",
    "# creating the document-term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# train LDA with 10 topics and print\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "lda = LdaModel(doc_term_matrix, num_topics=10, \n",
    "               id2word = dictionary, passes=3)\n",
    "lda.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-11T10:01:55.267Z"
    }
   },
   "outputs": [],
   "source": [
    "# to get the topic proportions for a document, use\n",
    "# the corresponding row from the document-term matrix.\n",
    "lda[doc_term_matrix[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-11T10:01:49.081Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# or, for all documents\n",
    "[lda[d] for d in doc_term_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:26:40.906548Z",
     "start_time": "2022-03-11T07:26:36.749256Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "# LDA Word Clouds\n",
    "###\n",
    "\n",
    "from numpy.random import randint\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make directory if not exists\n",
    "from os import mkdir\n",
    "try:\n",
    "    mkdir('lda')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# make word clouds for the topics\n",
    "for i,weights in lda.show_topics(num_topics=-1,\n",
    "                                 num_words=100,\n",
    "                                 formatted=False):\n",
    "    \n",
    "    #logweights = [w[0], np.log(w[1]) for w in weights]\n",
    "    maincol = randint(0,360)\n",
    "    def colorfunc(word=None, font_size=None, \n",
    "                  position=None, orientation=None, \n",
    "                  font_path=None, random_state=None):   \n",
    "        color = randint(maincol-10, maincol+10)\n",
    "        if color < 0:\n",
    "            color = 360 + color\n",
    "        return \"hsl(%d, %d%%, %d%%)\" % (color,randint(65, 75)+font_size / 7, randint(35, 45)-font_size / 10)   \n",
    "\n",
    "    \n",
    "    wordcloud = WordCloud(background_color=\"white\", \n",
    "                          ranks_only=False, \n",
    "                          max_font_size=120,\n",
    "                          color_func=colorfunc,\n",
    "                          height=600,width=800).generate_from_frequencies(dict(weights))\n",
    "\n",
    "    plt.clf()\n",
    "    plt.imshow(wordcloud,interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:16:19.464120Z",
     "start_time": "2022-03-11T07:16:17.547251Z"
    }
   },
   "outputs": [],
   "source": [
    "# pyLDAvis, for more details, refer to https://github.com/bmabey/pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(lda, doc_term_matrix, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Mallet to calculate coherence scores for different number of topics to automatically determine the best number of topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T08:37:27.411606Z",
     "start_time": "2022-03-11T08:31:06.461743Z"
    }
   },
   "outputs": [],
   "source": [
    "# you need gensim version <= 3.8.3 for this to work\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "mallet_path = '~/Downloads/mallet-2.0.8/bin/mallet'\n",
    "scores = []\n",
    "for num_topics in range(2, 20, 2):\n",
    "    print (num_topics)\n",
    "    lda = LdaMallet(mallet_path, doc_term_matrix, num_topics=num_topics, id2word=dictionary)\n",
    "    coherence = CoherenceModel(model=lda, texts=doc_clean, corpus=doc_term_matrix, dictionary=dictionary, coherence='c_v')\n",
    "    scores.append((num_topics, coherence.get_coherence()))\n",
    "pd.DataFrame(scores, columns=[\"Number of Topics\", \"Coherence Scores\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "For further reference for this and the following section see [here](https://github.com/fastai/course-nlp/blob/219d0c217bd83339e21471d31cd787e86d6ec0a0/2-svd-nmf-topic-modeling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:30:57.630094Z",
     "start_time": "2022-03-11T07:30:56.943444Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "\n",
    "X = pd.read_pickle('X.pkl').todense()\n",
    "vec = pd.read_pickle('vec-3grams-1.pkl')\n",
    "vocab = np.array(vec.get_feature_names())\n",
    "vocab[400:500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:31:00.112146Z",
     "start_time": "2022-03-11T07:30:59.201038Z"
    }
   },
   "outputs": [],
   "source": [
    "U, s, Vh = linalg.svd(X, full_matrices=False)\n",
    "print(U.shape, s.shape, Vh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:31:01.693951Z",
     "start_time": "2022-03-11T07:31:01.399719Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:31:03.071259Z",
     "start_time": "2022-03-11T07:31:03.057501Z"
    }
   },
   "outputs": [],
   "source": [
    "num_top_words=8\n",
    "\n",
    "def show_topics(a):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' '.join(t) for t in topic_words]\n",
    "\n",
    "show_topics(Vh[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization (NMF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:31:06.882377Z",
     "start_time": "2022-03-11T07:31:04.922245Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "clf = decomposition.NMF(n_components=10, random_state=1)\n",
    "\n",
    "W1 = clf.fit_transform(X)\n",
    "H1 = clf.components_\n",
    "\n",
    "show_topics(H1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:31:09.147221Z",
     "start_time": "2022-03-11T07:31:09.077828Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.test.utils import temporary_file\n",
    "\n",
    "df = df.reset_index()\n",
    "df['id'] = df.index\n",
    "author2doc = df[:100][['authorship','id']]\n",
    "author2doc = author2doc.groupby('authorship').apply(lambda x: list(x['id'])).to_dict()\n",
    "author2doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:31:10.623422Z",
     "start_time": "2022-03-11T07:31:10.554968Z"
    }
   },
   "outputs": [],
   "source": [
    "model = AuthorTopicModel(\n",
    "        doc_term_matrix, author2doc=author2doc, id2word=dictionary, num_topics=10)\n",
    "\n",
    "# For each author list topic distribution\n",
    "author_vecs = [model.get_author_topics(author) for author in model.id2author.values()]\n",
    "author_vecs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
